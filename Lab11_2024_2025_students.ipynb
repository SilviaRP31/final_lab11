{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiVCdd7Bd8B9"
   },
   "source": [
    "# Lab 11 (Evaluable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BKN-r2ld8CE"
   },
   "source": [
    "We work at the AI department of a hospital. From the digestive area, they are interested to predict the evolution of the Acute Pancreatitis based on the patient data that has been gathered from different stages.\n",
    "Besides, the digestive area requires a nice User eXperience (UX) webapp that facilitates the new patient's data introduction, data exploration and prediction.\n",
    "\n",
    "In summary, you as part of the Data Science team should tackle the challenge including:\n",
    "- An exhaustive analysis of the data of patients introduced in the platform in the past.\n",
    "- The development of a predictive model to anticipate the evolution of the patient; in particular, to predict if the patient is going to have persistent failure, i.e. \"OF_PERSIS_ANYTIME\" variable takes value 1.\n",
    "- The creation of a streamlit app that allows you to view the results of the analysis and interact with the model.\n",
    "- Adding an explainability tab to the app so that all doctors can understand each prediction for every patient and a global interpretation of the model\n",
    "\n",
    "# Practice Information:\n",
    "**Due date:** By end of November, 26th (23:55h)\n",
    "\n",
    "**Submission procedure:** via Moodle.\n",
    "\n",
    "**Name:** Silvia RiaÃ±o Prado\n",
    "\n",
    "**NIA:** 241429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KkLuvEdxd8CG"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoCR7VJHd8CJ"
   },
   "outputs": [],
   "source": [
    "#To see all columns of the datasets\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdgBy51Hd8CK"
   },
   "source": [
    "To facilitate the process of training and evaluation, we have created a function to split and train the original dataset.\n",
    "\n",
    "**EX[1]** To evaluate the performance of a binary classification model, which is the appropriate visualization to determine the quality of the performance and the threshold value of the decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtbDguEvd8CL"
   },
   "source": [
    "**Solution:**\n",
    "By using Shap functions such as \"shap_values\", in the plot we can easily visualize how the data is divided, determining the distribution among columns of data and decide our own thresholds based on the plot of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kegqQLHJd8CL"
   },
   "source": [
    "# 0) Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNvoquZud8CM"
   },
   "source": [
    "## 0.1) Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFHoZYlHd8CM"
   },
   "outputs": [],
   "source": [
    "def split_and_train(features, stage_df, target, num_var, cat_var, model):\n",
    "    X = stage_df[features]\n",
    "    y = stage_df[[target]]\n",
    "\n",
    "\n",
    "    # Divide the dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_processed=X_train\n",
    "\n",
    "    print(\"Number of features after coding:\", X_train_processed.shape[1] )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "\n",
    "    # Verify if the model has `feature_importances_`\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Obtain the importance of the features\n",
    "        importances = model.feature_importances_\n",
    "        # Create a dataframe with the feature_importance\n",
    "        num_features = X_train.shape[1]\n",
    "        feature_indices = np.arange(num_features)\n",
    "        importance_df = pd.DataFrame({'Feature': feature_indices, 'Importance': importances})\n",
    "        importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "        # Visualize the importance of the features\n",
    "\n",
    "        plt.figure(figsize=(10, 12))\n",
    "        plt.barh(X_train.columns[importance_df['Feature']], importance_df['Importance'], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Index of feature')\n",
    "        plt.title('Importance of the feature')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"The model does not have the property of 'feature_importances_'\")\n",
    "\n",
    "\n",
    "    return model, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FBtVcXFd8CN"
   },
   "source": [
    "**[EX2]** Besides, we have created another function to evaluate the model. Please, complete the function to plot the density charts of the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYV1Cr_dd8CO"
   },
   "source": [
    "## 0.2) Model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT6-EIzhd8CP"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, X_test, y_test, target, threshold):\n",
    "\n",
    "    X_test_processed=X_test\n",
    "    # Prediction for X_test\n",
    "    y_test_predict = model.predict(X_test_processed)\n",
    "\n",
    "    print(confusion_matrix(y_test, y_test_predict))\n",
    "    print(classification_report(y_test, y_test_predict))\n",
    "\n",
    "    # Prediction of the probabilities\n",
    "    y_test_predict_proba = model.predict_proba(X_test_processed)\n",
    "    y_pred=model.predict(X_test_processed)\n",
    "\n",
    "    # AUC calculation\n",
    "    y_test_prob = y_test_predict_proba[:, 1]  # Probabilidades de la clase positiva\n",
    "    auc = roc_auc_score(y_test, y_test_prob)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    ##TO DO: Plot of the probabilities distribution\n",
    "\n",
    "    y_test_total = pd.DataFrame({\n",
    "        target: y_test.values.flatten(),\n",
    "        'proba1': y_test_prob\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(y_test_total[y_test_total[target] == 0]['proba1'], label='Class 0', fill=True, color='red', alpha=0.5)\n",
    "    sns.kdeplot(y_test_total[y_test_total[target] == 1]['proba1'], label='Class 1', fill=True, color='blue', alpha=0.5)\n",
    "    plt.axvline(x=threshold, color='black', linestyle='--', label=f'Threshold: {threshold}')\n",
    "    plt.title('Density Plot of Predicted Probabilities')\n",
    "    plt.xlabel('Predicted Probability of Positive Class')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ##\n",
    "\n",
    "    # To adjust the prediction according to the threshold value\n",
    "    y_test_total['Predict'] = (y_test_total['proba1'] >= threshold).astype(int)\n",
    "\n",
    "    print(confusion_matrix(y_test_total[target], y_test_total[\"Predict\"]))\n",
    "    print(classification_report(y_test_total[target], y_test_total[\"Predict\"]))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBPTk8k_d8CQ"
   },
   "source": [
    "# 1) Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQUHx3THd8CQ"
   },
   "source": [
    "We upload the dataset from the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "7j7kvgNld8CR",
    "outputId": "9dc4c170-0578-49c0-a266-31a1084507e1"
   },
   "outputs": [],
   "source": [
    "# Import dataframe\n",
    "stage2_df = pd.read_csv('Stage_2.csv', sep=\";\")\n",
    "stage2_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PttLXtTDd8CT"
   },
   "source": [
    "# 2) Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhbrN6YQd8CU"
   },
   "source": [
    "## 2.1) Feature classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84eh7BkCd8CU"
   },
   "outputs": [],
   "source": [
    "#Columns selection\n",
    "columns_to_remove_stage2=[\"ID_PATIENT\", \"OF_PERSIS_ANYTIME\"]\n",
    "stage_2_columns=stage2_df.columns\n",
    "stage_2_columns=[col for col in stage_2_columns if col not in columns_to_remove_stage2]\n",
    "\n",
    "# numerical & categorical for stage0:\n",
    "num_variables_stage2 = stage2_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_variables_stage2 = [var for var in num_variables_stage2 if var not in columns_to_remove_stage2]\n",
    "\n",
    "categorical_variables_stage2 = stage2_df.select_dtypes(include='object').columns.tolist()\n",
    "categorical_variables_stage2 = [var for var in categorical_variables_stage2 if var not in columns_to_remove_stage2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5fCloYud8CV"
   },
   "source": [
    "## 2.2) Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu3Z0BnKd8CV"
   },
   "outputs": [],
   "source": [
    "num_variables_stage2.append(\"OF_PERSIS_ANYTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vh6pK9td8CW"
   },
   "source": [
    "**[EX3]** Calculate the correlation matrix and plot it of the numerical columns. Which are the top 3 variables with more correlation with \"OF_PERSIS_ANYTIME\" variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lap5vGVId8CW",
    "outputId": "e906b58d-fa91-4ee9-cb8f-344d249d60ac"
   },
   "outputs": [],
   "source": [
    "##Solution\n",
    "\n",
    "#numerical columns only\n",
    "numerical_columns = stage2_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "correlation_matrix = stage2_df[numerical_columns].corr()\n",
    "\n",
    "target_corr = correlation_matrix[\"OF_PERSIS_ANYTIME\"].drop(\"OF_PERSIS_ANYTIME\")  # Exclude self-correlation\n",
    "\n",
    "#plot correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', cbar=True)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "9n_dOSand8CX",
    "outputId": "c34a3a20-e432-412b-85e0-0b1b332f8e8f"
   },
   "outputs": [],
   "source": [
    "#Solution top 3 correlated\n",
    "top_3_corr = target_corr.abs().sort_values(ascending=False).head(3)\n",
    "top_3_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3HN1uNWd8CY"
   },
   "source": [
    "**Solution** The variables that correlates more with OF_PERSIS_ANYTIME are:\n",
    "SATO2%/FIO2%_ADMISS\twith 0.237365\n",
    "GLASGOWMENTAL\twith 0.198291\n",
    "BACTERIALI\twith 0.183964\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFiKhtm3d8CY"
   },
   "source": [
    "# 3) Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHslg9GWd8CZ"
   },
   "source": [
    "Let's train our model using the defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTkrKhRId8CZ"
   },
   "outputs": [],
   "source": [
    "num_variables_stage2 = [var for var in num_variables_stage2 if var not in columns_to_remove_stage2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZYJO0bjd8Ca"
   },
   "outputs": [],
   "source": [
    "model_stage2_OFPA=RandomForestClassifier(n_estimators=107, random_state=42, class_weight=\"balanced\", min_samples_leaf=17, max_depth=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rfmeFXGAd8Cb",
    "outputId": "19026e60-f0a1-460e-f546-08a6a018d595"
   },
   "outputs": [],
   "source": [
    "#Training the model\n",
    "model_trained_OFPA, X_test, y_test=split_and_train (stage_2_columns, stage2_df, \"OF_PERSIS_ANYTIME\", num_variables_stage2, categorical_variables_stage2, model_stage2_OFPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYEI1WEYd8Cc"
   },
   "source": [
    "**EX[4]** Evaluate the model using the defined ***evaluation()*** function. In our use case, should we optimize the recall or the precision? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "79dYRM-Wd8Cd",
    "outputId": "27f7483a-52c5-47de-8f7d-758d0275b2b9"
   },
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "y_pred = evaluation(model_trained_OFPA, X_test, y_test, target=\"OF_PERSIS_ANYTIME\", threshold=0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d6J_bh_d8Cd"
   },
   "source": [
    "**Solution:**\n",
    "It better to optimize recall, in healthcare position is allways better to have a false positive than a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV6scAzCd8Cf"
   },
   "source": [
    "# 4) Explainability AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0ATscW-d8Cg"
   },
   "source": [
    "As an excellent data scientist, we cannot conclude our work without understanding how the model works. In this section of the project, we will apply SHAP as a technique to understand, debug and explain our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFDX1RgXd8Ch"
   },
   "outputs": [],
   "source": [
    "X_test[\"target\"]=y_test\n",
    "X_test[\"prediction\"]=y_pred\n",
    "X_test=X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMxhjuzud8Ci"
   },
   "source": [
    "## 4.1) Global explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwkJhv1ud8Cj"
   },
   "source": [
    "Train a Shap explainer and calculate the shap_values object for the X_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 43
    },
    "id": "lKcj_qaed8Ck",
    "outputId": "783e5c88-659c-4b37-8645-272a72ca8d04"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer=shap.Explainer(model_stage2_OFPA)\n",
    "shap_values_OFPA = explainer(X_test.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NTn9ZR8d8Cl"
   },
   "source": [
    "**[EX5]** Which is the shspe of shap_values_OFPA object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgsPfEO5d8Cm",
    "outputId": "15b3b57b-cb95-4402-c962-748dd11932d8"
   },
   "outputs": [],
   "source": [
    "##Solution\n",
    "shap_values_OFPA.shape #120 number of samples, 53 number of features and 2 number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWMS3g0Qd8Cn"
   },
   "source": [
    "To access the SHAP values of the class 1 we should use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoDRSFlPd8Co"
   },
   "outputs": [],
   "source": [
    "# Access to the SHAP values for class 1\n",
    "shap_values_OFPA_class_1 = shap_values_OFPA[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ-NfcY1d8Cp"
   },
   "source": [
    "**[EX6]** Which is the average prediction for all patients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUc0plbId8Cq",
    "outputId": "ff60f059-3b5f-437e-a99b-f643d29c6df5"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "shap_values_OFPA.base_values #for class 1 is 0.50597409 and class 2 is 0.49402591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vufGfDF2d8Cr"
   },
   "source": [
    "**[EX7]** Plot the summary plot for global explainability of the model. Which are your insights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "WtKNdudpd8Cr",
    "outputId": "4ea5c237-23d4-4a5e-d2cf-e3376fca364b"
   },
   "outputs": [],
   "source": [
    "#solution\n",
    "shap.summary_plot(shap_values_OFPA.values[:,:,1], X_test[num_variables_stage2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytIbMwVCq6ZI"
   },
   "source": [
    "The values that make the probability closer to 1 are those that are more distributed along the axis, such as HEARTRATED ADMISS, SATO2%/FI02% ADMISS, AGE, and PAM ADMISS. These features have been identified as critical drivers of the model's predictions, and analyzing their SHAP values reveals valuable insights into the classification process.\n",
    "Higher Heart Rate shows that have higher risk, same as age and pam admiss. Where as the SATO2%/FIO2% the lower amount the highest risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mupHEL0sd8Cs"
   },
   "source": [
    "**EX[8]** Build the bar plot for the global explainability of the model but just for the top 15 variables. Tip: use the max_display=15 in the corresponding plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "r01kVfS_d8Ct",
    "outputId": "e7dd3068-928d-488c-a566-1e084cfaf3d2"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "#Plot var: built for class 1 of all top 15 shap_values\n",
    "shap.plots.bar(shap_values_OFPA[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG2BBGnsd8Cu"
   },
   "source": [
    "**EX[9]** Let's do deep dive in the variables `AGE`, `HEARTRATED_ADMISS` and `SATO2%/FIO2%_ADMISS`. What are the most relevant insights abour these evolution of the features' values and their Shap values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "RhlsffqXd8Cu",
    "outputId": "529d19e3-6364-4924-a416-8cad32c70bf9"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "shap.plots.scatter(shap_values_OFPA_class_1[:,\"AGE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "DCFMx2pXd8Cv",
    "outputId": "9d77c946-e234-421e-d00f-47055d442ef4"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "shap.plots.scatter(shap_values_OFPA_class_1[:,\"HEARTRATED_ADMISS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "7SHHolqDd8Cw",
    "outputId": "13b7768f-1803-43ab-ccc7-0cafc0c50f97"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "shap.plots.scatter(shap_values_OFPA_class_1[:,\"SATO2%/FIO2%_ADMISS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SpRnt_0d8Cw"
   },
   "source": [
    "### Local explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2DW7wUVd8Cx"
   },
   "source": [
    "Local explainability facilitates the understanding of the prediction for some particular cases. In other words, XAI closes to a personalized prediction explainability. Let's use the first sample of X_test for the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcOJ2qh3d8Cx"
   },
   "source": [
    "**EX[10]** Plot the waterfall and decision plot for two patients: one with prediction of being class 0 (i.e. No OF_PERSIST_ANYTIME) and another with prediction of being class 1 (i.e. OF_PERSIST_ANYTIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "5Bf1v4rWd8Cx",
    "outputId": "59ad806b-6419-4e00-b9ec-e0c0183109b2"
   },
   "outputs": [],
   "source": [
    "#Solution for a patient with prediction of being class 0\n",
    "y_pred_proba = model_stage2_OFPA.predict_proba(X_test[num_variables_stage2])\n",
    "\n",
    "patient_class_0_idx = np.where(y_pred_proba[:, 0] > 0.5)[0][0]\n",
    "shap_values_class_0 = shap_values_OFPA[patient_class_0_idx]\n",
    "\n",
    "shap.plots.waterfall(shap_values_class_0[ :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "fRlSC3zUd8Cy",
    "outputId": "ee48622e-cb71-4b7c-ad79-6b0807e6c0fb"
   },
   "outputs": [],
   "source": [
    "#Solution for a patient with prediction of being class 1\n",
    "patient_class_1_idx = np.where(y_pred_proba[:, 1] > 0.5)[0][0]\n",
    "shap_values_class_1 = shap_values_OFPA[patient_class_1_idx]\n",
    "shap.plots.waterfall(shap_values_class_1[ :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6oY9if0d8Cz"
   },
   "source": [
    "**EX[11]** Which are the factors that increase the risk of OF_PERSIST_ANYTIME??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC8cTF9Ld8C0"
   },
   "source": [
    "**Solution:**\n",
    "As we can see in the second plot, that refers to the OF_PERSIST_ANTIME risk, the factors that increse the risk are the ones in red that are: BMI, DIURETICO SATO2%/FIO2%_ADMISS TEMPERAPACHE_ADMISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpwFLacyd8C0"
   },
   "source": [
    "# 5) Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxswmVHSd8C1"
   },
   "source": [
    "As next step you create your own webapp. Store your prediciton model using pickle as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2AFvfdkd8C1",
    "outputId": "30e19ac3-0730-436d-cf86-9caea35b85ce"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el modelo en un archivo\n",
    "filename = 'OF_PERSIS_ANYTIME_classification_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model_trained_OFPA, file)\n",
    "\n",
    "print(f\"Your model has been stored as '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSQ2Jw2wd8C2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOr2Zlsad8C2"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
